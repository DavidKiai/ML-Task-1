# -*- coding: utf-8 -*-
"""ML Task 1-Feature Engineering Dimensionality

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZlmNL44qbznftnSYk9mzzb4u7jeNLvND
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn import linear_model,metrics
from sklearn.linear_model import Lasso,LassoCV
from sklearn.model_selection import train_test_split,KFold,cross_val_score,GridSearchCV,RandomizedSearchCV

"""# Prepare and Upload the Dataset"""

from google.colab import files
 
 
uploaded = files.upload()

#mounting the dataset

dataset = pd.read_csv("modified_data.csv")

#preview the dataset

dataset

#info on the dataset

dataset.info

#define the dataset shape

dataset.shape

"""# Fill in the Missing Values"""

bfill,mean_fill=[],[]

#return the missing values
dataset.isnull().sum()

#return duplicate values only
dataset.duplicated().any()

dataset['LotFrontage'] = dataset['LotFrontage'].fillna(dataset['LotFrontage'].mean())

dataset.isnull().sum()

"""# Encoding the Data"""

#picking categorical data
dataset.dtypes

#categorical data
cdata=dataset.select_dtypes(include=['object']).copy()
cdata.head(2)

#categorical features
cfeatures=list(cdata.columns)
cfeatures

#defining the two types of categorical features
nominal=['MSZoning','LandContour','LotConfig','Neighborhood']
ordinal=list(set(cfeatures)-set(nominal))
original_features=list(dataset.columns)
numerical=list(set(original_features)-set(cfeatures))
target=['SalePrice']

#return the Target(SalePrice)
dataset[target]

#convert categorical data to nominal data
for feature in ordinal:
  dataset[feature]=(dataset[feature].astype('category')).cat.codes
  df_nominal=pd.get_dummies(dataset[nominal])
  df_ordinal=dataset[ordinal]
  df_numerical=dataset[numerical]
  encoded_data=pd.concat([df_numerical,df_nominal,df_ordinal],axis=1)

new_data = encoded_data=pd.concat([df_numerical,df_nominal,df_ordinal],axis=1)
new_data = ['SalePrice']

#confirm if the shape is intact
new_data.shape

#return the encoded data
new_data

"""# Standardize the Dataset"""

#target feature
new_data.drop('SalePrice',axis=1,inplace=True)

#check the new dataset
new_data

numpy_array = new_data.to_numpy()
numpy_array

X=encoded_data.drop('SalePrice',axis=1)
y=dataset[target]
X=X.to_numpy()
y=y.to_numpy()

X.shape

y.shape

from sklearn.preprocessing import StandardScaler
X=StandardScaler().fit_transform(X)
y=StandardScaler().fit_transform(y)

y

X

print(np.any(np.isnan(X)))

x = np.nan_to_num(X)
print(x)



#standardize the dataset
standardized_data=StandardScaler().fit_transform(numpy_array)

standardized_data[0]

target_feature=dataset[target]
target_feature

#convert taregtdata to numparray()
target_feature=target_feature.to_numpy()

#define the shape of Y
target_feature.shape

target_feature
target_feature=StandardScaler().fit_transform(target_feature)

#standardized target feauture
target_feature

standardized_data.shape

"""# Feature Extraction using PCA"""

#number of components being extracted
pca=PCA(n_components=2)
p_components=pca.fit_transform(x)

p_components

#getting the ratio of variance
pca.explained_variance_ratio_

"""#Feature Selection"""

#finding the L1 coefficients for the columns
regressor = LassoCV();
pca_data = pd.DataFrame(p_components)
pca_data
regressor.fit(pca_data, np.ravel(target_feature))

coef  = pd.Series(regressor.coef_, index = pca_data.columns)
num_of_selected_features = sum(coef != 0)
num_of_selected_features

"""# Training the Model of the new dataset"""

reg=LassoCV()

pca = PCA(n_components = 2)
p_components = pca.fit_transform(x)
pca_data = pd.DataFrame(p_components)

reg.fit(pca_data, np.ravel(target_feature))

reg.coef_